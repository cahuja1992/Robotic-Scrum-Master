{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "from SpeechData import SpeechData\n",
    "from tensorflow.python.ops import ctc_ops\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# idx = tf.where(tf.not_equal(input_label, 0))\n",
    "# # Use tf.shape(a_t, out_type=tf.int64) instead of a_t.get_shape() if tensor shape is dynamic\n",
    "# sparse_labels = tf.SparseTensor(idx, tf.gather_nd(input_label, idx), tf.shape(input_label, out_type=tf.int64))\n",
    "\n",
    "\n",
    "\n",
    "# gather_nd is taken from https://github.com/tensorflow/tensorflow/issues/206#issuecomment-229678962\n",
    "# \n",
    "# Unfortunately we can't just use tf.gather_nd because it does not have gradients\n",
    "# implemented yet, so we need this workaround.\n",
    "#\n",
    "def gather_nd(params, indices, shape):\n",
    "    rank = len(shape)\n",
    "    flat_params = tf.reshape(params, [-1])\n",
    "    multipliers = [reduce(lambda x, y: x*y, shape[i+1:], 1) for i in range(0, rank)]\n",
    "    indices_unpacked = tf.unstack(tf.transpose(indices, [rank - 1] + range(0, rank - 1)))\n",
    "    flat_indices = sum([a*b for a,b in zip(multipliers, indices_unpacked)])\n",
    "    return tf.gather(flat_params, flat_indices)\n",
    "\n",
    "# ctc_label_dense_to_sparse is taken from https://github.com/tensorflow/tensorflow/issues/1742#issuecomment-205291527\n",
    "#\n",
    "# The CTC implementation in TensorFlow needs labels in a sparse representation,\n",
    "# but sparse data and queues don't mix well, so we store padded tensors in the\n",
    "# queue and convert to a sparse representation after dequeuing a batch.\n",
    "\n",
    "def ctc_label_dense_to_sparse(labels, label_lengths, batch_size):\n",
    "    # The second dimension of labels must be equal to the longest label length in the batch\n",
    "    correct_shape_assert = tf.assert_equal(tf.shape(labels)[1], tf.reduce_max(label_lengths))\n",
    "    with tf.control_dependencies([correct_shape_assert]):\n",
    "        labels = tf.identity(labels)\n",
    "\n",
    "    label_shape = tf.shape(labels)\n",
    "    num_batches_tns = tf.stack([label_shape[0]])\n",
    "    max_num_labels_tns = tf.stack([label_shape[1]])\n",
    "    def range_less_than(previous_state, current_input):\n",
    "        return tf.expand_dims(tf.range(label_shape[1]), 0) < current_input\n",
    "\n",
    "    init = tf.cast(tf.fill(max_num_labels_tns, 0), tf.bool)\n",
    "    init = tf.expand_dims(init, 0)\n",
    "    dense_mask = tf.scan(range_less_than, label_lengths, initializer=init, parallel_iterations=1)\n",
    "    dense_mask = dense_mask[:, 0, :]\n",
    "\n",
    "    label_array = tf.reshape(tf.tile(tf.range(0, label_shape[1]), num_batches_tns),\n",
    "          label_shape)\n",
    "    label_ind = tf.boolean_mask(label_array, dense_mask)\n",
    "\n",
    "    batch_array = tf.transpose(tf.reshape(tf.tile(tf.range(0, label_shape[0]), max_num_labels_tns), tf.reverse(label_shape, [0])))\n",
    "    batch_ind = tf.boolean_mask(batch_array, dense_mask)\n",
    "\n",
    "    indices = tf.transpose(tf.reshape(tf.concat([batch_ind, label_ind], 0), [2, -1]))\n",
    "    shape = [batch_size, tf.reduce_max(label_lengths)]\n",
    "    vals_sparse = gather_nd(labels, indices, shape)\n",
    "    \n",
    "    return tf.SparseTensor(tf.to_int64(indices), vals_sparse, tf.to_int64(label_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base path from where data will be read: SpeechData/\n",
      "Loading the speech metadata from path: SpeechData/\n",
      "Load data from enabled directory with training flag on : ['SpeechData/train/1/', 'SpeechData/train/2/']\n",
      "3195\n",
      "Wrong media files:\n",
      "[]\n",
      "wav file length after validation\n",
      "3195\n",
      "Length CHeck Failed: \n",
      "[]\n",
      "Wrong transcription. contains numeric or special character\n",
      "[]\n",
      "set([])\n",
      "set([])\n",
      "3195\n",
      "3195\n",
      "num_batch 199\n",
      "Epoch: 0  Step: 0\n",
      "Step: 0  Loss: 2012.21\n",
      "Epoch: 0  Step: 1\n",
      "Step: 1  Loss: 1916.31\n",
      "Epoch: 0  Step: 2\n",
      "Step: 2  Loss: 1641.97\n",
      "Epoch: 0  Step: 3\n",
      "Step: 3  Loss: 1264.85\n",
      "Epoch: 0  Step: 4\n",
      "Step: 4  Loss: 921.767\n",
      "Epoch: 0  Step: 5\n",
      "Step: 5  Loss: 764.682\n",
      "Epoch: 0  Step: 6\n",
      "Step: 6  Loss: 478.825\n",
      "Epoch: 0  Step: 7\n",
      "Step: 7  Loss: 169.54\n",
      "Epoch: 0  Step: 8\n",
      "Step: 8  Loss: 137.999\n",
      "Epoch: 0  Step: 9\n",
      "Step: 9  Loss: 181.184\n",
      "Epoch: 0  Step: 10\n",
      "Step: 10  Loss: 113.739\n",
      "Epoch: 0  Step: 11\n",
      "Step: 11  Loss: 146.989\n",
      "Epoch: 0  Step: 12\n",
      "Step: 12  Loss: 171.294\n",
      "Epoch: 0  Step: 13\n",
      "Step: 13  Loss: 215.249\n",
      "Epoch: 0  Step: 14\n",
      "Step: 14  Loss: 155.448\n",
      "Epoch: 0  Step: 15\n",
      "Step: 15  Loss: 164.599\n",
      "Epoch: 0  Step: 16\n",
      "Step: 16  Loss: 181.402\n",
      "Epoch: 0  Step: 17\n",
      "Step: 17  Loss: 178.532\n",
      "Epoch: 0  Step: 18\n",
      "Step: 18  Loss: 171.897\n",
      "Epoch: 0  Step: 19\n",
      "Step: 19  Loss: 193.032\n",
      "Epoch: 0  Step: 20\n",
      "Step: 20  Loss: 177.578\n",
      "Epoch: 0  Step: 21\n",
      "Step: 21  Loss: 151.721\n",
      "Epoch: 0  Step: 22\n",
      "Step: 22  Loss: 147.495\n",
      "Epoch: 0  Step: 23\n",
      "Step: 23  Loss: 101.822\n",
      "Epoch: 0  Step: 24\n",
      "Step: 24  Loss: 145.741\n",
      "Epoch: 0  Step: 25\n",
      "Step: 25  Loss: 119.359\n",
      "Epoch: 0  Step: 26\n",
      "Step: 26  Loss: 111.466\n",
      "Epoch: 0  Step: 27\n",
      "Step: 27  Loss: 123.414\n",
      "Epoch: 0  Step: 28\n",
      "Step: 28  Loss: 99.5362\n",
      "Epoch: 0  Step: 29\n",
      "Step: 29  Loss: 112.821\n",
      "Epoch: 0  Step: 30\n",
      "Step: 30  Loss: 97.3553\n",
      "Epoch: 0  Step: 31\n",
      "Step: 31  Loss: 110.532\n",
      "Epoch: 0  Step: 32\n",
      "Step: 32  Loss: 90.4666\n",
      "Epoch: 0  Step: 33\n",
      "Step: 33  Loss: 90.1127\n",
      "Epoch: 0  Step: 34\n",
      "Step: 34  Loss: 105.575\n",
      "Epoch: 0  Step: 35\n",
      "Step: 35  Loss: 99.7359\n",
      "Epoch: 0  Step: 36\n",
      "Step: 36  Loss: 97.865\n",
      "Epoch: 0  Step: 37\n",
      "Step: 37  Loss: 106.714\n",
      "Epoch: 0  Step: 38\n",
      "Step: 38  Loss: 103.797\n",
      "Epoch: 0  Step: 39\n",
      "Step: 39  Loss: 110.641\n",
      "Epoch: 0  Step: 40\n",
      "Step: 40  Loss: 97.7738\n",
      "Epoch: 0  Step: 41\n",
      "Step: 41  Loss: 113.92\n",
      "Epoch: 0  Step: 42\n",
      "Step: 42  Loss: 107.938\n",
      "Epoch: 0  Step: 43\n",
      "Step: 43  Loss: 118.504\n",
      "Epoch: 0  Step: 44\n",
      "Step: 44  Loss: 102.214\n",
      "Epoch: 0  Step: 45\n",
      "Step: 45  Loss: 103.687\n",
      "Epoch: 0  Step: 46\n",
      "Step: 46  Loss: 100.09\n",
      "Epoch: 0  Step: 47\n",
      "Step: 47  Loss: 100.096\n",
      "Epoch: 0  Step: 48\n",
      "Step: 48  Loss: 100.424\n",
      "Epoch: 0  Step: 49\n",
      "Step: 49  Loss: 89.8301\n",
      "Epoch: 0  Step: 50\n",
      "Step: 50  Loss: 101.209\n",
      "Epoch: 0  Step: 51\n",
      "Step: 51  Loss: 93.4534\n",
      "Epoch: 0  Step: 52\n",
      "Step: 52  Loss: 100.54\n",
      "Epoch: 0  Step: 53\n",
      "Step: 53  Loss: 90.7672\n",
      "Epoch: 0  Step: 54\n",
      "Step: 54  Loss: 90.2699\n",
      "Epoch: 0  Step: 55\n",
      "Step: 55  Loss: 98.004\n",
      "Epoch: 0  Step: 56\n",
      "Step: 56  Loss: 88.1932\n",
      "Epoch: 0  Step: 57\n",
      "Step: 57  Loss: 85.1414\n",
      "Epoch: 0  Step: 58\n",
      "Step: 58  Loss: 99.7782\n",
      "Epoch: 0  Step: 59\n",
      "Step: 59  Loss: 118.376\n",
      "Epoch: 0  Step: 60\n",
      "Step: 60  Loss: 92.5948\n",
      "Epoch: 0  Step: 61\n",
      "Step: 61  Loss: 92.6855\n",
      "Epoch: 0  Step: 62\n",
      "Step: 62  Loss: 108.922\n",
      "Epoch: 0  Step: 63\n",
      "Step: 63  Loss: 108.989\n",
      "Epoch: 0  Step: 64\n",
      "Step: 64  Loss: 76.3775\n",
      "Epoch: 0  Step: 65\n",
      "Step: 65  Loss: 69.4537\n",
      "Epoch: 0  Step: 66\n",
      "Step: 66  Loss: 98.942\n",
      "Epoch: 0  Step: 67\n",
      "Step: 67  Loss: 85.2757\n",
      "Epoch: 0  Step: 68\n",
      "Step: 68  Loss: 102.212\n",
      "Epoch: 0  Step: 69\n",
      "Step: 69  Loss: 85.7299\n",
      "Epoch: 0  Step: 70\n",
      "Step: 70  Loss: 100.18\n",
      "Epoch: 0  Step: 71\n",
      "Step: 71  Loss: 101.6\n",
      "Epoch: 0  Step: 72\n",
      "Step: 72  Loss: 89.6769\n",
      "Epoch: 0  Step: 73\n",
      "Step: 73  Loss: 89.8712\n",
      "Epoch: 0  Step: 74\n",
      "Step: 74  Loss: 82.5027\n",
      "Epoch: 0  Step: 75\n",
      "Step: 75  Loss: 95.4767\n",
      "Epoch: 0  Step: 76\n",
      "Step: 76  Loss: 83.8181\n",
      "Epoch: 0  Step: 77\n",
      "Step: 77  Loss: 99.7899\n",
      "Epoch: 0  Step: 78\n",
      "Step: 78  Loss: 80.4974\n",
      "Epoch: 0  Step: 79\n",
      "Step: 79  Loss: 86.8449\n",
      "Epoch: 0  Step: 80\n",
      "Step: 80  Loss: 90.3571\n",
      "Epoch: 0  Step: 81\n",
      "Step: 81  Loss: 93.6999\n",
      "Epoch: 0  Step: 82\n",
      "Step: 82  Loss: 75.6314\n",
      "Epoch: 0  Step: 83\n",
      "Step: 83  Loss: 90.5071\n",
      "Epoch: 0  Step: 84\n",
      "Step: 84  Loss: 93.3571\n",
      "Epoch: 0  Step: 85\n",
      "Step: 85  Loss: 97.1481\n",
      "Epoch: 0  Step: 86\n",
      "Step: 86  Loss: 89.5748\n",
      "Epoch: 0  Step: 87\n",
      "Step: 87  Loss: 101.881\n",
      "Epoch: 0  Step: 88\n",
      "Step: 88  Loss: 79.592\n",
      "Epoch: 0  Step: 89\n",
      "Step: 89  Loss: 90.0994\n",
      "Epoch: 0  Step: 90\n",
      "Step: 90  Loss: 95.7508\n",
      "Epoch: 0  Step: 91\n",
      "Step: 91  Loss: 80.6501\n",
      "Epoch: 0  Step: 92\n",
      "Step: 92  Loss: 104.42\n",
      "Epoch: 0  Step: 93\n",
      "Step: 93  Loss: 90.8255\n",
      "Epoch: 0  Step: 94\n",
      "Step: 94  Loss: 89.7662\n",
      "Epoch: 0  Step: 95\n",
      "Step: 95  Loss: 93.3856\n",
      "Epoch: 0  Step: 96\n",
      "Step: 96  Loss: 90.3132\n",
      "Epoch: 0  Step: 97\n",
      "Step: 97  Loss: 75.3058\n",
      "Epoch: 0  Step: 98\n",
      "Step: 98  Loss: 94.278\n",
      "Epoch: 0  Step: 99\n",
      "Step: 99  Loss: 107.754\n",
      "Epoch: 0  Step: 100\n",
      "Step: 100  Loss: 96.9667\n",
      "Epoch: 0  Step: 101\n",
      "Step: 101  Loss: 83.105\n",
      "Epoch: 0  Step: 102\n",
      "Step: 102  Loss: 94.981\n",
      "Epoch: 0  Step: 103\n",
      "Step: 103  Loss: 75.4287\n",
      "Epoch: 0  Step: 104\n",
      "Step: 104  Loss: 88.6957\n",
      "Epoch: 0  Step: 105\n",
      "Step: 105  Loss: 80.7249\n",
      "Epoch: 0  Step: 106\n",
      "Step: 106  Loss: 86.8869\n",
      "Epoch: 0  Step: 107\n",
      "Step: 107  Loss: 80.3016\n",
      "Epoch: 0  Step: 108\n",
      "Step: 108  Loss: 83.6272\n",
      "Epoch: 0  Step: 109\n",
      "Step: 109  Loss: 67.1882\n",
      "Epoch: 0  Step: 110\n",
      "Step: 110  Loss: 83.5922\n",
      "Epoch: 0  Step: 111\n",
      "Step: 111  Loss: 77.0914\n",
      "Epoch: 0  Step: 112\n",
      "Step: 112  Loss: 85.5509\n",
      "Epoch: 0  Step: 113\n",
      "Step: 113  Loss: 104.565\n",
      "Epoch: 0  Step: 114\n",
      "Step: 114  Loss: 86.1968\n",
      "Epoch: 0  Step: 115\n",
      "Step: 115  Loss: 108.477\n",
      "Epoch: 0  Step: 116\n",
      "Step: 116  Loss: 82.2344\n",
      "Epoch: 0  Step: 117\n",
      "Step: 117  Loss: 93.2732\n",
      "Epoch: 0  Step: 118\n",
      "Step: 118  Loss: 73.9501\n",
      "Epoch: 0  Step: 119\n",
      "Step: 119  Loss: 81.6206\n",
      "Epoch: 0  Step: 120\n",
      "Step: 120  Loss: 86.4965\n",
      "Epoch: 0  Step: 121\n",
      "Step: 121  Loss: 96.091\n",
      "Epoch: 0  Step: 122\n",
      "Step: 122  Loss: 79.2466\n",
      "Epoch: 0  Step: 123\n",
      "Step: 123  Loss: 83.4897\n",
      "Epoch: 0  Step: 124\n",
      "Step: 124  Loss: 91.7803\n",
      "Epoch: 0  Step: 125\n",
      "Step: 125  Loss: 88.3848\n",
      "Epoch: 0  Step: 126\n",
      "Step: 126  Loss: 113.157\n",
      "Epoch: 0  Step: 127\n",
      "Step: 127  Loss: 84.4376\n",
      "Epoch: 0  Step: 128\n",
      "Step: 128  Loss: 89.9136\n",
      "Epoch: 0  Step: 129\n",
      "Step: 129  Loss: 102.103\n",
      "Epoch: 0  Step: 130\n",
      "Step: 130  Loss: 79.3053\n",
      "Epoch: 0  Step: 131\n",
      "Step: 131  Loss: 94.143\n",
      "Epoch: 0  Step: 132\n",
      "Step: 132  Loss: 100.427\n",
      "Epoch: 0  Step: 133\n",
      "Step: 133  Loss: 90.0768\n",
      "Epoch: 0  Step: 134\n",
      "Step: 134  Loss: 82.2137\n",
      "Epoch: 0  Step: 135\n",
      "Step: 135  Loss: 79.4902\n",
      "Epoch: 0  Step: 136\n",
      "Step: 136  Loss: 86.4885\n",
      "Epoch: 0  Step: 137\n",
      "Step: 137  Loss: 102.616\n",
      "Epoch: 0  Step: 138\n",
      "Step: 138  Loss: 95.1833\n",
      "Epoch: 0  Step: 139\n",
      "Step: 139  Loss: 79.6432\n",
      "Epoch: 0  Step: 140\n",
      "Step: 140  Loss: 102.977\n",
      "Epoch: 0  Step: 141\n",
      "Step: 141  Loss: 93.0437\n",
      "Epoch: 0  Step: 142\n",
      "Step: 142  Loss: 107.001\n",
      "Epoch: 0  Step: 143\n",
      "Step: 143  Loss: 94.0943\n",
      "Epoch: 0  Step: 144\n",
      "Step: 144  Loss: 98.034\n",
      "Epoch: 0  Step: 145\n",
      "Step: 145  Loss: 86.5494\n",
      "Epoch: 0  Step: 146\n",
      "Step: 146  Loss: 99.5499\n",
      "Epoch: 0  Step: 147\n",
      "Step: 147  Loss: 84.6682\n",
      "Epoch: 0  Step: 148\n",
      "Step: 148  Loss: 96.5212\n",
      "Epoch: 0  Step: 149\n",
      "Step: 149  Loss: 86.39\n",
      "Epoch: 0  Step: 150\n",
      "Step: 150  Loss: 82.5991\n",
      "Epoch: 0  Step: 151\n",
      "Step: 151  Loss: 96.4354\n",
      "Epoch: 0  Step: 152\n",
      "Step: 152  Loss: 104.135\n",
      "Epoch: 0  Step: 153\n",
      "Step: 153  Loss: 87.6735\n",
      "Epoch: 0  Step: 154\n",
      "Step: 154  Loss: 92.348\n",
      "Epoch: 0  Step: 155\n",
      "Step: 155  Loss: 81.8769\n",
      "Epoch: 0  Step: 156\n",
      "Step: 156  Loss: 96.004\n",
      "Epoch: 0  Step: 157\n",
      "Step: 157  Loss: 77.2451\n",
      "Epoch: 0  Step: 158\n",
      "Step: 158  Loss: 98.8916\n",
      "Epoch: 0  Step: 159\n",
      "Step: 159  Loss: 96.7485\n",
      "Epoch: 0  Step: 160\n",
      "Step: 160  Loss: 82.9586\n",
      "Epoch: 0  Step: 161\n",
      "Step: 161  Loss: 97.3469\n",
      "Epoch: 0  Step: 162\n",
      "Step: 162  Loss: 96.0827\n",
      "Epoch: 0  Step: 163\n",
      "Step: 163  Loss: 96.5818\n",
      "Epoch: 0  Step: 164\n",
      "Step: 164  Loss: 94.1548\n",
      "Epoch: 0  Step: 165\n",
      "Step: 165  Loss: 82.7282\n",
      "Epoch: 0  Step: 166\n",
      "Step: 166  Loss: 101.025\n",
      "Epoch: 0  Step: 167\n",
      "Step: 167  Loss: 85.0089\n",
      "Epoch: 0  Step: 168\n",
      "Step: 168  Loss: 87.3968\n",
      "Epoch: 0  Step: 169\n",
      "Step: 169  Loss: 86.2732\n",
      "Epoch: 0  Step: 170\n",
      "Step: 170  Loss: 87.7469\n",
      "Epoch: 0  Step: 171\n",
      "Step: 171  Loss: 94.9294\n",
      "Epoch: 0  Step: 172\n",
      "Step: 172  Loss: 99.2385\n",
      "Epoch: 0  Step: 173\n",
      "Step: 173  Loss: 88.3351\n",
      "Epoch: 0  Step: 174\n",
      "Step: 174  Loss: 94.3493\n",
      "Epoch: 0  Step: 175\n",
      "Step: 175  Loss: 83.5815\n",
      "Epoch: 0  Step: 176\n",
      "Step: 176  Loss: 99.0445\n",
      "Epoch: 0  Step: 177\n",
      "Step: 177  Loss: 96.0346\n",
      "Epoch: 0  Step: 178\n",
      "Step: 178  Loss: 92.9352\n",
      "Epoch: 0  Step: 179\n",
      "Step: 179  Loss: 87.3842\n",
      "Epoch: 0  Step: 180\n",
      "Step: 180  Loss: 100.624\n",
      "Epoch: 0  Step: 181\n",
      "Step: 181  Loss: 99.0434\n",
      "Epoch: 0  Step: 182\n",
      "Step: 182  Loss: 93.6099\n",
      "Epoch: 0  Step: 183\n",
      "Step: 183  Loss: 81.8752\n",
      "Epoch: 0  Step: 184\n",
      "Step: 184  Loss: 84.6319\n",
      "Epoch: 0  Step: 185\n",
      "Step: 185  Loss: 91.9213\n",
      "Epoch: 0  Step: 186\n",
      "Step: 186  Loss: 89.5253\n",
      "Epoch: 0  Step: 187\n",
      "Step: 187  Loss: 87.2645\n",
      "Epoch: 0  Step: 188\n",
      "Step: 188  Loss: 90.9375\n",
      "Epoch: 0  Step: 189\n",
      "Step: 189  Loss: 103.038\n",
      "Epoch: 0  Step: 190\n",
      "Step: 190  Loss: 101.323\n",
      "Epoch: 0  Step: 191\n",
      "Step: 191  Loss: 81.0486\n",
      "Epoch: 0  Step: 192\n",
      "Step: 192  Loss: 82.2013\n",
      "Epoch: 0  Step: 193\n",
      "Step: 193  Loss: 90.7165\n",
      "Epoch: 0  Step: 194\n",
      "Step: 194  Loss: 100.755\n",
      "Epoch: 0  Step: 195\n",
      "Step: 195  Loss: 79.5679\n",
      "Epoch: 0  Step: 196\n",
      "Step: 196  Loss: 77.0965\n",
      "Epoch: 0  Step: 197\n",
      "Step: 197  Loss: 80.7689\n",
      "Epoch: 0  Step: 198\n",
      "Step: 198  Loss: 89.7253\n",
      "Epoch: 0  AccLoss: 28061.9519653\n",
      "Epoch: 1  Step: 0\n",
      "Step: 0  Loss: 94.0477\n",
      "Epoch: 1  Step: 1\n",
      "Step: 1  Loss: 73.5036\n",
      "Epoch: 1  Step: 2\n",
      "Step: 2  Loss: 81.1118\n",
      "Epoch: 1  Step: 3\n",
      "Step: 3  Loss: 83.8986\n",
      "Epoch: 1  Step: 4\n",
      "Step: 4  Loss: 90.8467\n",
      "Epoch: 1  Step: 5\n",
      "Step: 5  Loss: 81.9306\n",
      "Epoch: 1  Step: 6\n",
      "Step: 6  Loss: 93.2018\n",
      "Epoch: 1  Step: 7\n",
      "Step: 7  Loss: 89.8569\n",
      "Epoch: 1  Step: 8\n",
      "Step: 8  Loss: 94.1172\n",
      "Epoch: 1  Step: 9\n",
      "Step: 9  Loss: 88.6922\n",
      "Epoch: 1  Step: 10\n",
      "Step: 10  Loss: 87.1609\n",
      "Epoch: 1  Step: 11\n",
      "Step: 11  Loss: 85.0249\n",
      "Epoch: 1  Step: 12\n",
      "Step: 12  Loss: 82.1993\n",
      "Epoch: 1  Step: 13\n",
      "Step: 13  Loss: 80.1498\n",
      "Epoch: 1  Step: 14\n",
      "Step: 14  Loss: 94.8357\n",
      "Epoch: 1  Step: 15\n",
      "Step: 15  Loss: 74.2955\n",
      "Epoch: 1  Step: 16\n",
      "Step: 16  Loss: 87.767\n",
      "Epoch: 1  Step: 17\n",
      "Step: 17  Loss: 89.3717\n",
      "Epoch: 1  Step: 18\n",
      "Step: 18  Loss: 96.5295\n",
      "Epoch: 1  Step: 19\n",
      "Step: 19  Loss: 86.8136\n",
      "Epoch: 1  Step: 20\n",
      "Step: 20  Loss: 95.0907\n",
      "Epoch: 1  Step: 21\n",
      "Step: 21  Loss: 83.3183\n",
      "Epoch: 1  Step: 22\n",
      "Step: 22  Loss: 95.9977\n",
      "Epoch: 1  Step: 23\n",
      "Step: 23  Loss: 94.9202\n",
      "Epoch: 1  Step: 24\n",
      "Step: 24  Loss: 84.7494\n",
      "Epoch: 1  Step: 25\n",
      "Step: 25  Loss: 88.1893\n",
      "Epoch: 1  Step: 26\n",
      "Step: 26  Loss: 91.905\n",
      "Epoch: 1  Step: 27\n",
      "Step: 27  Loss: 78.5829\n",
      "Epoch: 1  Step: 28\n",
      "Step: 28  Loss: 91.3975\n",
      "Epoch: 1  Step: 29\n",
      "Step: 29  Loss: 82.4769\n",
      "Epoch: 1  Step: 30\n",
      "Step: 30  Loss: 99.0605\n",
      "Epoch: 1  Step: 31\n",
      "Step: 31  Loss: 100.541\n",
      "Epoch: 1  Step: 32\n",
      "Step: 32  Loss: 90.9423\n",
      "Epoch: 1  Step: 33\n",
      "Step: 33  Loss: 92.8612\n",
      "Epoch: 1  Step: 34\n",
      "Step: 34  Loss: 94.57\n",
      "Epoch: 1  Step: 35\n",
      "Step: 35  Loss: 93.6532\n",
      "Epoch: 1  Step: 36\n",
      "Step: 36  Loss: 96.1242\n",
      "Epoch: 1  Step: 37\n",
      "Step: 37  Loss: 96.4253\n",
      "Epoch: 1  Step: 38\n",
      "Step: 38  Loss: 89.2394\n",
      "Epoch: 1  Step: 39\n",
      "Step: 39  Loss: 90.7036\n",
      "Epoch: 1  Step: 40\n",
      "Step: 40  Loss: 93.4524\n",
      "Epoch: 1  Step: 41\n",
      "Step: 41  Loss: 84.9204\n",
      "Epoch: 1  Step: 42\n",
      "Step: 42  Loss: 96.5802\n",
      "Epoch: 1  Step: 43\n",
      "Step: 43  Loss: 81.2598\n",
      "Epoch: 1  Step: 44\n",
      "Step: 44  Loss: 88.5886\n",
      "Epoch: 1  Step: 45\n",
      "Step: 45  Loss: 91.9889\n",
      "Epoch: 1  Step: 46\n",
      "Step: 46  Loss: 84.7318\n",
      "Epoch: 1  Step: 47\n",
      "Step: 47  Loss: 99.3771\n",
      "Epoch: 1  Step: 48\n",
      "Step: 48  Loss: 72.7757\n",
      "Epoch: 1  Step: 49\n",
      "Step: 49  Loss: 87.3747\n",
      "Epoch: 1  Step: 50\n",
      "Step: 50  Loss: 86.2746\n",
      "Epoch: 1  Step: 51\n",
      "Step: 51  Loss: 99.173\n",
      "Epoch: 1  Step: 52\n",
      "Step: 52  Loss: 86.3567\n",
      "Epoch: 1  Step: 53\n",
      "Step: 53  Loss: 90.8241\n",
      "Epoch: 1  Step: 54\n",
      "Step: 54  Loss: 102.066\n",
      "Epoch: 1  Step: 55\n",
      "Step: 55  Loss: 91.5296\n",
      "Epoch: 1  Step: 56\n",
      "Step: 56  Loss: 87.5286\n",
      "Epoch: 1  Step: 57\n",
      "Step: 57  Loss: 88.5191\n",
      "Epoch: 1  Step: 58\n",
      "Step: 58  Loss: 75.8653\n",
      "Epoch: 1  Step: 59\n",
      "Step: 59  Loss: 101.637\n",
      "Epoch: 1  Step: 60\n",
      "Step: 60  Loss: 79.5947\n",
      "Epoch: 1  Step: 61\n",
      "Step: 61  Loss: 88.6952\n",
      "Epoch: 1  Step: 62\n",
      "Step: 62  Loss: 92.3292\n",
      "Epoch: 1  Step: 63\n",
      "Step: 63  Loss: 113.119\n",
      "Epoch: 1  Step: 64\n",
      "Step: 64  Loss: 93.0157\n",
      "Epoch: 1  Step: 65\n",
      "Step: 65  Loss: 104.985\n",
      "Epoch: 1  Step: 66\n",
      "Step: 66  Loss: 88.0911\n",
      "Epoch: 1  Step: 67\n",
      "Step: 67  Loss: 75.4838\n",
      "Epoch: 1  Step: 68\n",
      "Step: 68  Loss: 108.394\n",
      "Epoch: 1  Step: 69\n",
      "Step: 69  Loss: 93.2359\n",
      "Epoch: 1  Step: 70\n",
      "Step: 70  Loss: 89.1288\n",
      "Epoch: 1  Step: 71\n",
      "Step: 71  Loss: 100.132\n",
      "Epoch: 1  Step: 72\n",
      "Step: 72  Loss: 81.71\n",
      "Epoch: 1  Step: 73\n",
      "Step: 73  Loss: 95.7611\n",
      "Epoch: 1  Step: 74\n",
      "Step: 74  Loss: 88.4093\n",
      "Epoch: 1  Step: 75\n",
      "Step: 75  Loss: 78.4359\n",
      "Epoch: 1  Step: 76\n",
      "Step: 76  Loss: 84.0751\n",
      "Epoch: 1  Step: 77\n",
      "Step: 77  Loss: 80.0257\n",
      "Epoch: 1  Step: 78\n",
      "Step: 78  Loss: 77.4558\n",
      "Epoch: 1  Step: 79\n",
      "Step: 79  Loss: 93.1425\n",
      "Epoch: 1  Step: 80\n",
      "Step: 80  Loss: 90.5762\n",
      "Epoch: 1  Step: 81\n",
      "Step: 81  Loss: 89.9324\n",
      "Epoch: 1  Step: 82\n",
      "Step: 82  Loss: 78.7237\n",
      "Epoch: 1  Step: 83\n",
      "Step: 83  Loss: 115.655\n",
      "Epoch: 1  Step: 84\n",
      "Step: 84  Loss: 104.955\n",
      "Epoch: 1  Step: 85\n",
      "Step: 85  Loss: 101.981\n",
      "Epoch: 1  Step: 86\n",
      "Step: 86  Loss: 83.5297\n",
      "Epoch: 1  Step: 87\n",
      "Step: 87  Loss: 96.8885\n",
      "Epoch: 1  Step: 88\n",
      "Step: 88  Loss: 95.4457\n",
      "Epoch: 1  Step: 89\n",
      "Step: 89  Loss: 84.9906\n",
      "Epoch: 1  Step: 90\n",
      "Step: 90  Loss: 74.3829\n",
      "Epoch: 1  Step: 91\n",
      "Step: 91  Loss: 91.6551\n",
      "Epoch: 1  Step: 92\n",
      "Step: 92  Loss: 86.7943\n",
      "Epoch: 1  Step: 93\n",
      "Step: 93  Loss: 87.9226\n",
      "Epoch: 1  Step: 94\n",
      "Step: 94  Loss: 68.5329\n",
      "Epoch: 1  Step: 95\n",
      "Step: 95  Loss: 98.8144\n",
      "Epoch: 1  Step: 96\n",
      "Step: 96  Loss: 101.724\n",
      "Epoch: 1  Step: 97\n",
      "Step: 97  Loss: 81.4799\n",
      "Epoch: 1  Step: 98\n",
      "Step: 98  Loss: 86.2247\n",
      "Epoch: 1  Step: 99\n",
      "Step: 99  Loss: 89.6124\n",
      "Epoch: 1  Step: 100\n",
      "Step: 100  Loss: 112.38\n",
      "Epoch: 1  Step: 101\n",
      "Step: 101  Loss: 75.5914\n",
      "Epoch: 1  Step: 102\n",
      "Step: 102  Loss: 96.7103\n",
      "Epoch: 1  Step: 103\n",
      "Step: 103  Loss: 77.8345\n",
      "Epoch: 1  Step: 104\n",
      "Step: 104  Loss: 84.633\n",
      "Epoch: 1  Step: 105\n",
      "Step: 105  Loss: 83.9138\n",
      "Epoch: 1  Step: 106\n",
      "Step: 106  Loss: 106.263\n",
      "Epoch: 1  Step: 107\n",
      "Step: 107  Loss: 83.3311\n",
      "Epoch: 1  Step: 108\n",
      "Step: 108  Loss: 90.3217\n",
      "Epoch: 1  Step: 109\n",
      "Step: 109  Loss: 102.326\n",
      "Epoch: 1  Step: 110\n",
      "Step: 110  Loss: 92.2172\n",
      "Epoch: 1  Step: 111\n",
      "Step: 111  Loss: 85.5183\n",
      "Epoch: 1  Step: 112\n",
      "Step: 112  Loss: 85.2197\n",
      "Epoch: 1  Step: 113\n",
      "Step: 113  Loss: 96.1547\n",
      "Epoch: 1  Step: 114\n",
      "Step: 114  Loss: 105.661\n",
      "Epoch: 1  Step: 115\n",
      "Step: 115  Loss: 85.0309\n",
      "Epoch: 1  Step: 116\n",
      "Step: 116  Loss: 97.3797\n",
      "Epoch: 1  Step: 117\n",
      "Step: 117  Loss: 86.4601\n",
      "Epoch: 1  Step: 118\n",
      "Step: 118  Loss: 91.5037\n",
      "Epoch: 1  Step: 119\n",
      "Step: 119  Loss: 75.9632\n",
      "Epoch: 1  Step: 120\n",
      "Step: 120  Loss: 83.7035\n",
      "Epoch: 1  Step: 121\n",
      "Step: 121  Loss: 95.3922\n",
      "Epoch: 1  Step: 122\n",
      "Step: 122  Loss: 82.4642\n",
      "Epoch: 1  Step: 123\n",
      "Step: 123  Loss: 92.6829\n",
      "Epoch: 1  Step: 124\n",
      "Step: 124  Loss: 74.7684\n",
      "Epoch: 1  Step: 125\n",
      "Step: 125  Loss: 92.2965\n",
      "Epoch: 1  Step: 126\n",
      "Step: 126  Loss: 75.0701\n",
      "Epoch: 1  Step: 127\n",
      "Step: 127  Loss: 101.332\n",
      "Epoch: 1  Step: 128\n",
      "Step: 128  Loss: 95.677\n",
      "Epoch: 1  Step: 129\n",
      "Step: 129  Loss: 106.763\n",
      "Epoch: 1  Step: 130\n",
      "Step: 130  Loss: 97.9554\n",
      "Epoch: 1  Step: 131\n",
      "Step: 131  Loss: 92.4607\n",
      "Epoch: 1  Step: 132\n",
      "Step: 132  Loss: 93.5398\n",
      "Epoch: 1  Step: 133\n",
      "Step: 133  Loss: 88.9501\n",
      "Epoch: 1  Step: 134\n",
      "Step: 134  Loss: 100.353\n",
      "Epoch: 1  Step: 135\n",
      "Step: 135  Loss: 99.3763\n",
      "Epoch: 1  Step: 136\n",
      "Step: 136  Loss: 102.587\n",
      "Epoch: 1  Step: 137\n",
      "Step: 137  Loss: 88.4184\n",
      "Epoch: 1  Step: 138\n",
      "Step: 138  Loss: 95.3838\n",
      "Epoch: 1  Step: 139\n",
      "Step: 139  Loss: 83.9657\n",
      "Epoch: 1  Step: 140\n",
      "Step: 140  Loss: 90.425\n",
      "Epoch: 1  Step: 141\n",
      "Step: 141  Loss: 84.3981\n",
      "Epoch: 1  Step: 142\n",
      "Step: 142  Loss: 90.0203\n",
      "Epoch: 1  Step: 143\n",
      "Step: 143  Loss: 75.5759\n",
      "Epoch: 1  Step: 144\n",
      "Step: 144  Loss: 85.0407\n",
      "Epoch: 1  Step: 145\n",
      "Step: 145  Loss: 80.9203\n",
      "Epoch: 1  Step: 146\n",
      "Step: 146  Loss: 88.1712\n",
      "Epoch: 1  Step: 147\n",
      "Step: 147  Loss: 87.7012\n",
      "Epoch: 1  Step: 148\n",
      "Step: 148  Loss: 96.7423\n",
      "Epoch: 1  Step: 149\n",
      "Step: 149  Loss: 70.7783\n",
      "Epoch: 1  Step: 150\n",
      "Step: 150  Loss: 106.234\n",
      "Epoch: 1  Step: 151\n",
      "Step: 151  Loss: 102.389\n",
      "Epoch: 1  Step: 152\n",
      "Step: 152  Loss: 83.8693\n",
      "Epoch: 1  Step: 153\n",
      "Step: 153  Loss: 92.3689\n",
      "Epoch: 1  Step: 154\n",
      "Step: 154  Loss: 90.4338\n",
      "Epoch: 1  Step: 155\n",
      "Step: 155  Loss: 86.051\n",
      "Epoch: 1  Step: 156\n",
      "Step: 156  Loss: 79.8049\n",
      "Epoch: 1  Step: 157\n",
      "Step: 157  Loss: 101.915\n",
      "Epoch: 1  Step: 158\n",
      "Step: 158  Loss: 92.8867\n",
      "Epoch: 1  Step: 159\n",
      "Step: 159  Loss: 99.5227\n",
      "Epoch: 1  Step: 160\n",
      "Step: 160  Loss: 91.0901\n",
      "Epoch: 1  Step: 161\n",
      "Step: 161  Loss: 93.9079\n",
      "Epoch: 1  Step: 162\n",
      "Step: 162  Loss: 81.0416\n",
      "Epoch: 1  Step: 163\n",
      "Step: 163  Loss: 87.1616\n",
      "Epoch: 1  Step: 164\n",
      "Step: 164  Loss: 93.6561\n",
      "Epoch: 1  Step: 165\n",
      "Step: 165  Loss: 85.9146\n",
      "Epoch: 1  Step: 166\n",
      "Step: 166  Loss: 83.9864\n",
      "Epoch: 1  Step: 167\n",
      "Step: 167  Loss: 77.788\n",
      "Epoch: 1  Step: 168\n",
      "Step: 168  Loss: 78.5464\n",
      "Epoch: 1  Step: 169\n",
      "Step: 169  Loss: 88.9377\n",
      "Epoch: 1  Step: 170\n",
      "Step: 170  Loss: 84.4213\n",
      "Epoch: 1  Step: 171\n",
      "Step: 171  Loss: 84.0484\n",
      "Epoch: 1  Step: 172\n",
      "Step: 172  Loss: 84.1614\n",
      "Epoch: 1  Step: 173\n",
      "Step: 173  Loss: 87.179\n",
      "Epoch: 1  Step: 174\n",
      "Step: 174  Loss: 93.6303\n",
      "Epoch: 1  Step: 175\n",
      "Step: 175  Loss: 85.8409\n",
      "Epoch: 1  Step: 176\n",
      "Step: 176  Loss: 105.257\n",
      "Epoch: 1  Step: 177\n",
      "Step: 177  Loss: 85.9015\n",
      "Epoch: 1  Step: 178\n",
      "Step: 178  Loss: 95.5198\n",
      "Epoch: 1  Step: 179\n",
      "Step: 179  Loss: 97.2892\n",
      "Epoch: 1  Step: 180\n",
      "Step: 180  Loss: 88.7842\n",
      "Epoch: 1  Step: 181\n",
      "Step: 181  Loss: 83.9233\n",
      "Epoch: 1  Step: 182\n",
      "Step: 182  Loss: 85.2925\n",
      "Epoch: 1  Step: 183\n",
      "Step: 183  Loss: 86.2701\n",
      "Epoch: 1  Step: 184\n",
      "Step: 184  Loss: 81.9848\n",
      "Epoch: 1  Step: 185\n",
      "Step: 185  Loss: 93.0877\n",
      "Epoch: 1  Step: 186\n",
      "Step: 186  Loss: 92.2994\n",
      "Epoch: 1  Step: 187\n",
      "Step: 187  Loss: 68.7412\n",
      "Epoch: 1  Step: 188\n",
      "Step: 188  Loss: 80.9506\n",
      "Epoch: 1  Step: 189\n",
      "Step: 189  Loss: 100.668\n",
      "Epoch: 1  Step: 190\n",
      "Step: 190  Loss: 84.234\n",
      "Epoch: 1  Step: 191\n",
      "Step: 191  Loss: 87.3425\n",
      "Epoch: 1  Step: 192\n",
      "Step: 192  Loss: 106.93\n",
      "Epoch: 1  Step: 193\n",
      "Step: 193  Loss: 71.5791\n",
      "Epoch: 1  Step: 194\n",
      "Step: 194  Loss: 94.1645\n",
      "Epoch: 1  Step: 195\n",
      "Step: 195  Loss: 86.4056\n",
      "Epoch: 1  Step: 196\n",
      "Step: 196  Loss: 80.5082\n",
      "Epoch: 1  Step: 197\n",
      "Step: 197  Loss: 96.6217\n",
      "Epoch: 1  Step: 198\n",
      "Step: 198  Loss: 114.331\n",
      "Epoch: 1  AccLoss: 17864.1693649\n",
      "Epoch: 2  Step: 0\n",
      "Step: 0  Loss: 85.7353\n",
      "Epoch: 2  Step: 1\n",
      "Step: 1  Loss: 97.7009\n",
      "Epoch: 2  Step: 2\n",
      "Step: 2  Loss: 84.5386\n",
      "Epoch: 2  Step: 3\n",
      "Step: 3  Loss: 86.1555\n",
      "Epoch: 2  Step: 4\n",
      "Step: 4  Loss: 100.161\n",
      "Epoch: 2  Step: 5\n",
      "Step: 5  Loss: 89.7046\n",
      "Epoch: 2  Step: 6\n",
      "Step: 6  Loss: 95.5329\n",
      "Epoch: 2  Step: 7\n",
      "Step: 7  Loss: 88.1167\n",
      "Epoch: 2  Step: 8\n",
      "Step: 8  Loss: 110.097\n",
      "Epoch: 2  Step: 9\n",
      "Step: 9  Loss: 70.2653\n",
      "Epoch: 2  Step: 10\n",
      "Step: 10  Loss: 89.3336\n",
      "Epoch: 2  Step: 11\n",
      "Step: 11  Loss: 87.9492\n",
      "Epoch: 2  Step: 12\n",
      "Step: 12  Loss: 90.9126\n",
      "Epoch: 2  Step: 13\n",
      "Step: 13  Loss: 90.7872\n",
      "Epoch: 2  Step: 14\n",
      "Step: 14  Loss: 89.1596\n",
      "Epoch: 2  Step: 15\n",
      "Step: 15  Loss: 84.5871\n",
      "Epoch: 2  Step: 16\n",
      "Step: 16  Loss: 78.4557\n",
      "Epoch: 2  Step: 17\n",
      "Step: 17  Loss: 76.517\n",
      "Epoch: 2  Step: 18\n",
      "Step: 18  Loss: 87.5409\n",
      "Epoch: 2  Step: 19\n",
      "Step: 19  Loss: 75.8259\n",
      "Epoch: 2  Step: 20\n",
      "Step: 20  Loss: 89.6141\n",
      "Epoch: 2  Step: 21\n",
      "Step: 21  Loss: 92.598\n",
      "Epoch: 2  Step: 22\n",
      "Step: 22  Loss: 87.054\n",
      "Epoch: 2  Step: 23\n",
      "Step: 23  Loss: 81.3084\n",
      "Epoch: 2  Step: 24\n",
      "Step: 24  Loss: 89.9013\n",
      "Epoch: 2  Step: 25\n",
      "Step: 25  Loss: 95.4253\n",
      "Epoch: 2  Step: 26\n",
      "Step: 26  Loss: 85.9837\n",
      "Epoch: 2  Step: 27\n",
      "Step: 27  Loss: 115.412\n",
      "Epoch: 2  Step: 28\n",
      "Step: 28  Loss: 83.4874\n",
      "Epoch: 2  Step: 29\n",
      "Step: 29  Loss: 93.0884\n",
      "Epoch: 2  Step: 30\n",
      "Step: 30  Loss: 84.2107\n",
      "Epoch: 2  Step: 31\n",
      "Step: 31  Loss: 87.1299\n",
      "Epoch: 2  Step: 32\n",
      "Step: 32  Loss: 82.175\n",
      "Epoch: 2  Step: 33\n",
      "Step: 33  Loss: 82.7624\n",
      "Epoch: 2  Step: 34\n",
      "Step: 34  Loss: 90.3266\n",
      "Epoch: 2  Step: 35\n",
      "Step: 35  Loss: 106.07\n",
      "Epoch: 2  Step: 36\n",
      "Step: 36  Loss: 84.4252\n",
      "Epoch: 2  Step: 37\n",
      "Step: 37  Loss: 93.9584\n",
      "Epoch: 2  Step: 38\n",
      "Step: 38  Loss: 84.1531\n",
      "Epoch: 2  Step: 39\n",
      "Step: 39  Loss: 98.9005\n",
      "Epoch: 2  Step: 40\n",
      "Step: 40  Loss: 81.5886\n",
      "Epoch: 2  Step: 41\n",
      "Step: 41  Loss: 83.2818\n",
      "Epoch: 2  Step: 42\n",
      "Step: 42  Loss: 85.669\n",
      "Epoch: 2  Step: 43\n",
      "Step: 43  Loss: 91.6191\n",
      "Epoch: 2  Step: 44\n",
      "Step: 44  Loss: 92.218\n",
      "Epoch: 2  Step: 45\n",
      "Step: 45  Loss: 93.4678\n",
      "Epoch: 2  Step: 46\n",
      "Step: 46  Loss: 99.422\n",
      "Epoch: 2  Step: 47\n",
      "Step: 47  Loss: 90.6328\n",
      "Epoch: 2  Step: 48\n",
      "Step: 48  Loss: 109.582\n",
      "Epoch: 2  Step: 49\n",
      "Step: 49  Loss: 85.11\n",
      "Epoch: 2  Step: 50\n",
      "Step: 50  Loss: 85.7018\n",
      "Epoch: 2  Step: 51\n",
      "Step: 51  Loss: 89.7096\n",
      "Epoch: 2  Step: 52\n",
      "Step: 52  Loss: 90.1985\n",
      "Epoch: 2  Step: 53\n",
      "Step: 53  Loss: 80.3049\n",
      "Epoch: 2  Step: 54\n",
      "Step: 54  Loss: 78.5553\n",
      "Epoch: 2  Step: 55\n",
      "Step: 55  Loss: 108.799\n",
      "Epoch: 2  Step: 56\n",
      "Step: 56  Loss: 96.6437\n",
      "Epoch: 2  Step: 57\n",
      "Step: 57  Loss: 101.761\n",
      "Epoch: 2  Step: 58\n",
      "Step: 58  Loss: 90.3525\n",
      "Epoch: 2  Step: 59\n",
      "Step: 59  Loss: 107.242\n",
      "Epoch: 2  Step: 60\n",
      "Step: 60  Loss: 110.449\n",
      "Epoch: 2  Step: 61\n",
      "Step: 61  Loss: 85.9832\n",
      "Epoch: 2  Step: 62\n",
      "Step: 62  Loss: 86.6551\n",
      "Epoch: 2  Step: 63\n",
      "Step: 63  Loss: 87.4563\n",
      "Epoch: 2  Step: 64\n",
      "Step: 64  Loss: 97.925\n",
      "Epoch: 2  Step: 65\n",
      "Step: 65  Loss: 87.6137\n",
      "Epoch: 2  Step: 66\n",
      "Step: 66  Loss: 90.3815\n",
      "Epoch: 2  Step: 67\n",
      "Step: 67  Loss: 87.4621\n",
      "Epoch: 2  Step: 68\n",
      "Step: 68  Loss: 84.8223\n",
      "Epoch: 2  Step: 69\n",
      "Step: 69  Loss: 83.4971\n",
      "Epoch: 2  Step: 70\n",
      "Step: 70  Loss: 91.9396\n",
      "Epoch: 2  Step: 71\n",
      "Step: 71  Loss: 96.1106\n",
      "Epoch: 2  Step: 72\n",
      "Step: 72  Loss: 81.2311\n",
      "Epoch: 2  Step: 73\n",
      "Step: 73  Loss: 92.6862\n",
      "Epoch: 2  Step: 74\n",
      "Step: 74  Loss: 89.8863\n",
      "Epoch: 2  Step: 75\n",
      "Step: 75  Loss: 81.9483\n",
      "Epoch: 2  Step: 76\n",
      "Step: 76  Loss: 90.3864\n",
      "Epoch: 2  Step: 77\n",
      "Step: 77  Loss: 97.7329\n",
      "Epoch: 2  Step: 78\n",
      "Step: 78  Loss: 81.5864\n",
      "Epoch: 2  Step: 79\n",
      "Step: 79  Loss: 87.9059\n",
      "Epoch: 2  Step: 80\n",
      "Step: 80  Loss: 102.605\n",
      "Epoch: 2  Step: 81\n",
      "Step: 81  Loss: 98.4828\n",
      "Epoch: 2  Step: 82\n",
      "Step: 82  Loss: 92.1808\n",
      "Epoch: 2  Step: 83\n",
      "Step: 83  Loss: 102.565\n",
      "Epoch: 2  Step: 84\n",
      "Step: 84  Loss: 84.434\n",
      "Epoch: 2  Step: 85\n",
      "Step: 85  Loss: 77.4674\n",
      "Epoch: 2  Step: 86\n",
      "Step: 86  Loss: 86.9155\n",
      "Epoch: 2  Step: 87\n",
      "Step: 87  Loss: 91.2298\n",
      "Epoch: 2  Step: 88\n",
      "Step: 88  Loss: 96.0472\n",
      "Epoch: 2  Step: 89\n",
      "Step: 89  Loss: 72.3932\n",
      "Epoch: 2  Step: 90\n",
      "Step: 90  Loss: 94.7229\n",
      "Epoch: 2  Step: 91\n",
      "Step: 91  Loss: 88.1719\n",
      "Epoch: 2  Step: 92\n",
      "Step: 92  Loss: 84.6776\n",
      "Epoch: 2  Step: 93\n",
      "Step: 93  Loss: 88.657\n",
      "Epoch: 2  Step: 94\n",
      "Step: 94  Loss: 82.4147\n",
      "Epoch: 2  Step: 95\n",
      "Step: 95  Loss: 87.4796\n",
      "Epoch: 2  Step: 96\n",
      "Step: 96  Loss: 71.8042\n",
      "Epoch: 2  Step: 97\n",
      "Step: 97  Loss: 77.7505\n",
      "Epoch: 2  Step: 98\n",
      "Step: 98  Loss: 90.7078\n",
      "Epoch: 2  Step: 99\n",
      "Step: 99  Loss: 101.913\n",
      "Epoch: 2  Step: 100\n",
      "Step: 100  Loss: 86.7943\n",
      "Epoch: 2  Step: 101\n",
      "Step: 101  Loss: 79.2583\n",
      "Epoch: 2  Step: 102\n",
      "Step: 102  Loss: 99.1642\n",
      "Epoch: 2  Step: 103\n",
      "Step: 103  Loss: 99.3239\n",
      "Epoch: 2  Step: 104\n",
      "Step: 104  Loss: 73.4602\n",
      "Epoch: 2  Step: 105\n",
      "Step: 105  Loss: 93.2547\n",
      "Epoch: 2  Step: 106\n",
      "Step: 106  Loss: 81.5195\n",
      "Epoch: 2  Step: 107\n",
      "Step: 107  Loss: 82.6003\n",
      "Epoch: 2  Step: 108\n",
      "Step: 108  Loss: 75.0175\n",
      "Epoch: 2  Step: 109\n",
      "Step: 109  Loss: 89.9219\n",
      "Epoch: 2  Step: 110\n",
      "Step: 110  Loss: 98.1435\n",
      "Epoch: 2  Step: 111\n",
      "Step: 111  Loss: 97.0657\n",
      "Epoch: 2  Step: 112\n",
      "Step: 112  Loss: 90.5568\n",
      "Epoch: 2  Step: 113\n",
      "Step: 113  Loss: 96.6867\n",
      "Epoch: 2  Step: 114\n",
      "Step: 114  Loss: 80.751\n",
      "Epoch: 2  Step: 115\n",
      "Step: 115  Loss: 102.443\n",
      "Epoch: 2  Step: 116\n",
      "Step: 116  Loss: 90.9833\n",
      "Epoch: 2  Step: 117\n",
      "Step: 117  Loss: 78.0935\n",
      "Epoch: 2  Step: 118\n",
      "Step: 118  Loss: 85.9418\n",
      "Epoch: 2  Step: 119\n",
      "Step: 119  Loss: 95.0053\n",
      "Epoch: 2  Step: 120\n",
      "Step: 120  Loss: 100.675\n",
      "Epoch: 2  Step: 121\n",
      "Step: 121  Loss: 79.3953\n",
      "Epoch: 2  Step: 122\n",
      "Step: 122  Loss: 87.9199\n",
      "Epoch: 2  Step: 123\n",
      "Step: 123  Loss: 82.1677\n",
      "Epoch: 2  Step: 124\n",
      "Step: 124  Loss: 95.2047\n",
      "Epoch: 2  Step: 125\n",
      "Step: 125  Loss: 91.9852\n",
      "Epoch: 2  Step: 126\n",
      "Step: 126  Loss: 86.2284\n",
      "Epoch: 2  Step: 127\n",
      "Step: 127  Loss: 81.6382\n",
      "Epoch: 2  Step: 128\n",
      "Step: 128  Loss: 71.4418\n",
      "Epoch: 2  Step: 129\n",
      "Step: 129  Loss: 76.476\n",
      "Epoch: 2  Step: 130\n",
      "Step: 130  Loss: 79.8627\n",
      "Epoch: 2  Step: 131\n",
      "Step: 131  Loss: 82.4492\n",
      "Epoch: 2  Step: 132\n",
      "Step: 132  Loss: 76.3259\n",
      "Epoch: 2  Step: 133\n",
      "Step: 133  Loss: 105.776\n",
      "Epoch: 2  Step: 134\n",
      "Step: 134  Loss: 96.4467\n",
      "Epoch: 2  Step: 135\n",
      "Step: 135  Loss: 89.041\n",
      "Epoch: 2  Step: 136\n",
      "Step: 136  Loss: 93.7044\n",
      "Epoch: 2  Step: 137\n",
      "Step: 137  Loss: 79.9882\n",
      "Epoch: 2  Step: 138\n",
      "Step: 138  Loss: 75.9021\n",
      "Epoch: 2  Step: 139\n",
      "Step: 139  Loss: 86.3279\n",
      "Epoch: 2  Step: 140\n",
      "Step: 140  Loss: 90.0077\n",
      "Epoch: 2  Step: 141\n",
      "Step: 141  Loss: 89.595\n",
      "Epoch: 2  Step: 142\n",
      "Step: 142  Loss: 88.4274\n",
      "Epoch: 2  Step: 143\n",
      "Step: 143  Loss: 80.7759\n",
      "Epoch: 2  Step: 144\n",
      "Step: 144  Loss: 97.819\n",
      "Epoch: 2  Step: 145\n",
      "Step: 145  Loss: 92.0436\n",
      "Epoch: 2  Step: 146\n",
      "Step: 146  Loss: 71.6849\n",
      "Epoch: 2  Step: 147\n",
      "Step: 147  Loss: 92.7718\n",
      "Epoch: 2  Step: 148\n",
      "Step: 148  Loss: 98.4447\n",
      "Epoch: 2  Step: 149\n",
      "Step: 149  Loss: 82.1711\n",
      "Epoch: 2  Step: 150\n",
      "Step: 150  Loss: 76.4984\n",
      "Epoch: 2  Step: 151\n",
      "Step: 151  Loss: 103.235\n",
      "Epoch: 2  Step: 152\n",
      "Step: 152  Loss: 93.502\n",
      "Epoch: 2  Step: 153\n",
      "Step: 153  Loss: 76.8097\n",
      "Epoch: 2  Step: 154\n",
      "Step: 154  Loss: 82.6657\n",
      "Epoch: 2  Step: 155\n",
      "Step: 155  Loss: 90.6333\n",
      "Epoch: 2  Step: 156\n",
      "Step: 156  Loss: 87.5512\n",
      "Epoch: 2  Step: 157\n",
      "Step: 157  Loss: 79.7177\n",
      "Epoch: 2  Step: 158\n",
      "Step: 158  Loss: 91.8048\n",
      "Epoch: 2  Step: 159\n",
      "Step: 159  Loss: 93.1041\n",
      "Epoch: 2  Step: 160\n",
      "Step: 160  Loss: 86.8293\n",
      "Epoch: 2  Step: 161\n",
      "Step: 161  Loss: 79.1189\n",
      "Epoch: 2  Step: 162\n",
      "Step: 162  Loss: 73.2211\n",
      "Epoch: 2  Step: 163\n",
      "Step: 163  Loss: 74.3609\n",
      "Epoch: 2  Step: 164\n",
      "Step: 164  Loss: 97.2605\n",
      "Epoch: 2  Step: 165\n",
      "Step: 165  Loss: 81.516\n",
      "Epoch: 2  Step: 166\n",
      "Step: 166  Loss: 91.7392\n",
      "Epoch: 2  Step: 167\n",
      "Step: 167  Loss: 109.563\n",
      "Epoch: 2  Step: 168\n",
      "Step: 168  Loss: 88.4657\n",
      "Epoch: 2  Step: 169\n",
      "Step: 169  Loss: 87.4781\n",
      "Epoch: 2  Step: 170\n",
      "Step: 170  Loss: 68.0197\n",
      "Epoch: 2  Step: 171\n",
      "Step: 171  Loss: 79.9346\n",
      "Epoch: 2  Step: 172\n",
      "Step: 172  Loss: 96.3981\n",
      "Epoch: 2  Step: 173\n",
      "Step: 173  Loss: 81.9074\n",
      "Epoch: 2  Step: 174\n",
      "Step: 174  Loss: 86.1185\n",
      "Epoch: 2  Step: 175\n",
      "Step: 175  Loss: 92.0273\n",
      "Epoch: 2  Step: 176\n",
      "Step: 176  Loss: 79.6725\n",
      "Epoch: 2  Step: 177\n",
      "Step: 177  Loss: 78.7131\n",
      "Epoch: 2  Step: 178\n",
      "Step: 178  Loss: 99.4053\n",
      "Epoch: 2  Step: 179\n",
      "Step: 179  Loss: 95.1871\n",
      "Epoch: 2  Step: 180\n",
      "Step: 180  Loss: 99.6336\n",
      "Epoch: 2  Step: 181\n",
      "Step: 181  Loss: 83.2235\n",
      "Epoch: 2  Step: 182\n",
      "Step: 182  Loss: 92.6346\n",
      "Epoch: 2  Step: 183\n",
      "Step: 183  Loss: 78.5596\n",
      "Epoch: 2  Step: 184\n",
      "Step: 184  Loss: 99.5071\n",
      "Epoch: 2  Step: 185\n",
      "Step: 185  Loss: 92.3839\n",
      "Epoch: 2  Step: 186\n",
      "Step: 186  Loss: 97.8393\n",
      "Epoch: 2  Step: 187\n",
      "Step: 187  Loss: 95.2994\n",
      "Epoch: 2  Step: 188\n",
      "Step: 188  Loss: 76.6108\n",
      "Epoch: 2  Step: 189\n",
      "Step: 189  Loss: 103.559\n",
      "Epoch: 2  Step: 190\n",
      "Step: 190  Loss: 91.7744\n",
      "Epoch: 2  Step: 191\n",
      "Step: 191  Loss: 91.7206\n",
      "Epoch: 2  Step: 192\n",
      "Step: 192  Loss: 109.14\n",
      "Epoch: 2  Step: 193\n",
      "Step: 193  Loss: 86.691\n",
      "Epoch: 2  Step: 194\n",
      "Step: 194  Loss: 97.753\n",
      "Epoch: 2  Step: 195\n",
      "Step: 195  Loss: 92.3639\n",
      "Epoch: 2  Step: 196\n",
      "Step: 196  Loss: 91.1553\n",
      "Epoch: 2  Step: 197\n",
      "Step: 197  Loss: 101.865\n",
      "Epoch: 2  Step: 198\n",
      "Step: 198  Loss: 93.763\n",
      "Epoch: 2  AccLoss: 17731.8184586\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "# set log level to debug\n",
    "#tf.sg_verbosity(10)\n",
    "\n",
    "#\n",
    "# hyper parameters\n",
    "#\n",
    "batch_size = 16    # batch size\n",
    "num_blocks = 3     # dilated blocks\n",
    "num_dim = 128      # latent dimension\n",
    "initial_learning_rate = 0.001\n",
    "#\n",
    "# inputs\n",
    "#\n",
    "\n",
    "data = SpeechData(batch_size=batch_size, data_path='SpeechData/')\n",
    "\n",
    "# vocabulary size\n",
    "voca_size = data.voca_size\n",
    "\n",
    "# spectrum feature of audio\n",
    "input_features = data.features\n",
    "input_features_length = data.features_length\n",
    "\n",
    "# # target sentence label\n",
    "input_labels = data.labels\n",
    "input_labels_length = data.labels_length\n",
    "\n",
    "num_batch = data.num_batch\n",
    "\n",
    "\n",
    "print 'num_batch', num_batch\n",
    "\n",
    "# Reshape input speech data  [Batch, Height, Width, Channel]\n",
    "input_features = tf.reshape(input_features, shape=[batch_size, 81, -1, 1])\n",
    "#input_feature.get_shape()\n",
    "\n",
    "\n",
    "# Store layers weight & bias   --- Weights are called filter in convolution\n",
    "# 9*5 filter  but stride of 2,2  this will help to find overlap regions\n",
    "convolution_neuron_out_channels = 32\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([9, 5, 1, convolution_neuron_out_channels]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([convolution_neuron_out_channels]))\n",
    "}\n",
    "\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME', data_format=\"NHWC\")\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, shift):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, shift, shift, 1], strides=[1, shift, shift, 1], padding='SAME')\n",
    "\n",
    "\n",
    "# x = tf.reshape(x, shape=[-1, 81, None, 1])\n",
    "# Convolution Layer\n",
    "conv1 = conv2d(input_features, weights['wc1'], biases['bc1'], strides=2)\n",
    "\n",
    "conv1.get_shape()\n",
    "\n",
    "conv1 = maxpool2d(conv1, shift=2)\n",
    "\n",
    "conv1 = tf.transpose(conv1, perm=[0, 1, 3, 2])\n",
    "#Above shape is TensorShape([Dimension(16), Dimension(21), Dimension(32), Dimension(None)]): 9*5 filter 2*2 stride\n",
    "conv1 = tf.reshape(conv1, shape=[batch_size, convolution_neuron_out_channels, -1])\n",
    "#Above shape is TensorShape([Dimension(16), Dimension(32), Dimension(None)])\n",
    "#conv1 = tf.transpose(conv1, perm=[0, 2, 1])\n",
    "\n",
    "\n",
    "#conv1.get_shape()   \n",
    "#TensorShape([Dimension(16), , Dimension(32), Dimension(None)])\n",
    "\n",
    "# RNN Layers\n",
    "\n",
    "num_neurons = 200\n",
    "num_layers = 2\n",
    "dropout = 0.8\n",
    "# The number of characters in the target language plus one ===   (a-z + space + one extra)\n",
    "out_size = 28\n",
    "relu_clip = 20\n",
    "\n",
    "\n",
    "weight = tf.Variable(tf.truncated_normal([num_neurons, out_size], stddev=0.1))\n",
    "bias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n",
    "\n",
    "cell = tf.contrib.rnn.GRUCell(num_neurons)  # Or LSTMCell(num_neurons)\n",
    "cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "cell = tf.contrib.rnn.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "# Batch size x max_length x num_neurons.\n",
    "conv1 =  tf.transpose(conv1, perm=[0, 2, 1])\n",
    "output, state = tf.nn.dynamic_rnn(cell, conv1, dtype=tf.float32, time_major=False)\n",
    "\n",
    "#Batch size x max_length x num_neurons.\n",
    "max_length = tf.shape(output)[1]\n",
    "\n",
    "output = tf.reshape(output, [-1, num_neurons])\n",
    "prediction = tf.minimum(tf.nn.relu(tf.matmul(output, weight) + bias), relu_clip)\n",
    "prediction = tf.reshape(prediction, [batch_size, max_length, out_size])\n",
    "\n",
    "\n",
    "#CTC Layer\n",
    "\n",
    "#Dense to sparse vector conversion\n",
    "sparse_labels = ctc_label_dense_to_sparse(input_labels, input_labels_length, batch_size)\n",
    "\n",
    "#For every item in batch find the length of the batch\n",
    "transformed_sequence_length = tf.map_fn(lambda x: tf.shape(x)[0], prediction, dtype=(tf.int32))\n",
    "\n",
    "loss_from_ctc = ctc_ops.ctc_loss(inputs=prediction, labels=sparse_labels, sequence_length=transformed_sequence_length, time_major=False)\n",
    "\n",
    "\n",
    "#Backpropagation Layer\n",
    "cost = tf.reduce_mean(loss_from_ctc)\n",
    "optimizer = tf.train.AdamOptimizer(0.001).minimize(cost)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "    try:\n",
    "        for epoch in xrange(1):\n",
    "            epoch_loss = 0\n",
    "            for step in xrange(num_batch):\n",
    "                print 'Epoch:', epoch ,' Step:', step\n",
    "                _, c = sess.run([optimizer, cost])\n",
    "                print 'Step:', step, ' Loss:', c\n",
    "                epoch_loss += c\n",
    "            print 'Epoch:', epoch, ' AccLoss:', epoch_loss\n",
    "        print 'Finished.'\n",
    "    except Exception, e:\n",
    "        # Report exceptions to the coordinator.\n",
    "        coord.request_stop(e)\n",
    "    finally:\n",
    "        # stop our queue threads and properly close the session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
