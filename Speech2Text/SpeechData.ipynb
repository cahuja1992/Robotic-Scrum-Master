{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import itertools\n",
    "import data_validation\n",
    "import threading\n",
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "from tensorflow.python.platform import tf_logging as logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataReadAndValidate(object):\n",
    "    def __init__(self, data_path='SpeechData/'):\n",
    "        \n",
    "        print \"Base path from where data will be read: %s\" % (data_path)\n",
    "        self.label,self.wav = self._load_corpus(data_path)\n",
    "        \n",
    "\n",
    "    def _load_data_dir(self, data_path) :\n",
    "        #print data_path\n",
    "        all_train_wav_files = []\n",
    "        all_train_txt_files = []\n",
    "        for file in os.listdir(data_path + 'wav'):\n",
    "            if file.endswith(\".wav\"):\n",
    "                all_train_wav_files.extend([data_path + 'wav/' + file])\n",
    "        for file in os.listdir(data_path + 'txt'):\n",
    "            if file.endswith(\".txt\"):\n",
    "                all_train_txt_files.extend([data_path + 'txt/' + file])\n",
    "        return sorted(all_train_wav_files), sorted(all_train_txt_files)\n",
    "\n",
    "\n",
    "    def _load_data_from_locations(self, location_ids) :\n",
    "        all_train_wav_files = []\n",
    "        all_train_txt_files = []\n",
    "        for base_location in location_ids:\n",
    "            data1, data2 = self._load_data_dir(base_location)\n",
    "            all_train_wav_files.extend(data1)\n",
    "            all_train_txt_files.extend(data2)\n",
    "\n",
    "        return all_train_wav_files, all_train_txt_files\n",
    " \n",
    "\n",
    "\n",
    "    #Load data from directory. Data stored in format for Training \n",
    "    #train/<Number>/wav/ dir contains media files\n",
    "    #train/<Number>/txt/ dir contains transcription files\n",
    "\n",
    "    #Load data from directory. Data stored in format for Testing \n",
    "    #test/<Number>/wav/ dir contains media files\n",
    "    #test/<Number>/txt/ dir contains transcription files\n",
    "    \n",
    "    def _load_corpus(self, data_path):\n",
    "\n",
    "        print 'Loading the speech metadata from path:', data_path\n",
    "        # read meta-info\n",
    "        #INTERVIEW: Interviewer will provide you the meaning of this file\n",
    "        df = pd.read_table(data_path + 'data-info.txt', usecols=['ID','Enable','Train'],\n",
    "                           delimiter=',',index_col=False)\n",
    "\n",
    "        # collect train file ids\n",
    "        # make file ID\n",
    "        train_file_ids = []\n",
    "        test_file_ids = []\n",
    "        for index, row in df.iterrows():\n",
    "            if row['Enable'] == 'Y' and row['Train'] == 'Y':\n",
    "                train_file_ids.extend([data_path + 'train/' + str(row['ID']) + '/'])\n",
    "            if row['Enable'] == 'Y' and row['Train'] == 'N':\n",
    "                train_file_ids.extend([data_path + 'test/' + str(row['ID']) + '/'])\n",
    "\n",
    "\n",
    "        print 'Load data from enabled directory with training flag on :', train_file_ids\n",
    "        all_train_wav_files, all_train_txt_files = self._load_data_from_locations(train_file_ids)\n",
    "\n",
    "        #Validate the data --- Disable this of you are running again and again on same data\n",
    "        all_train_wav_files, all_train_txt_files = data_validation.validate_data(all_train_wav_files, all_train_txt_files)\n",
    "        all_train_wav_files = sorted(all_train_wav_files)\n",
    "        all_train_txt_files = sorted(all_train_txt_files)\n",
    "\n",
    "        return all_train_txt_files, all_train_wav_files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DictionaryBuilding(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        #The text files will only contains english lower case alphabet with space and 0 for padding\n",
    "        #creating byte to index dictionary\n",
    "        self.index2byte = [0] + [ord(' ')] + list(range(ord('a'), ord('z')+1))\n",
    "\n",
    "        self.byte2index = {}\n",
    "        for i, b in enumerate(self.index2byte):\n",
    "            self.byte2index[b] = i        \n",
    "        self.voca_size = len(self.index2byte)\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class _FuncQueueRunner(tf.train.QueueRunner):\n",
    "\n",
    "    def __init__(self, func, data_producer=None, queue=None, enqueue_ops=None, close_op=None,\n",
    "                 cancel_op=None, queue_closed_exception_types=None,\n",
    "                 queue_runner_def=None):\n",
    "        # save ad-hoc function\n",
    "        self.func = func\n",
    "        self.data_producer = data_producer\n",
    "        # call super()\n",
    "        super(_FuncQueueRunner, self).__init__(queue, enqueue_ops, close_op, cancel_op,\n",
    "                                               queue_closed_exception_types, queue_runner_def)\n",
    "\n",
    "    # pylint: disable=broad-except\n",
    "    def _run(self, sess, enqueue_op, coord=None):\n",
    "\n",
    "        if coord:\n",
    "            coord.register_thread(threading.current_thread())\n",
    "        decremented = False\n",
    "        try:\n",
    "            while True:\n",
    "                if coord and coord.should_stop():\n",
    "                    break\n",
    "                try:\n",
    "                    #CUSTOM FUNCTION CALL\n",
    "                    self.func(sess, enqueue_op, self.data_producer)  # call enqueue function\n",
    "                    #CUSTOM FUNCTION CALL\n",
    "                except self._queue_closed_exception_types:  # pylint: disable=catching-non-exception\n",
    "                    # This exception indicates that a queue was closed.\n",
    "                    with self._lock:\n",
    "                        self._runs_per_session[sess] -= 1\n",
    "                        decremented = True\n",
    "                        if self._runs_per_session[sess] == 0:\n",
    "                            try:\n",
    "                                sess.run(self._close_op)\n",
    "                            except Exception as e:\n",
    "                                # Intentionally ignore errors from close_op.\n",
    "                                logging.vlog(1, \"Ignored exception: %s\", str(e))\n",
    "                        return\n",
    "        except Exception as e:\n",
    "            # This catches all other exceptions.\n",
    "            if coord:\n",
    "                coord.request_stop(e)\n",
    "            else:\n",
    "                logging.error(\"Exception in QueueRunner: %s\", str(e))\n",
    "                with self._lock:\n",
    "                    self._exceptions_raised.append(e)\n",
    "                raise\n",
    "        finally:\n",
    "            # Make sure we account for all terminations: normal or errors.\n",
    "            if not decremented:\n",
    "                with self._lock:\n",
    "                    self._runs_per_session[sess] -= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SpeechData(object):\n",
    "\n",
    "    def __init__(self, batch_size=16, data_path='SpeechData/'):\n",
    "\n",
    "        #load dictionary\n",
    "        dictionary = DictionaryBuilding()\n",
    "        \n",
    "        self.byte2index = dictionary.byte2index\n",
    "        self.index2byte = dictionary.index2byte\n",
    "        self.voca_size = dictionary.voca_size\n",
    "\n",
    "        # Constants\n",
    "        SPACE_TOKEN = '<space>'\n",
    "        SPACE_INDEX = 0\n",
    "        FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
    "\n",
    "        def text_to_char_array(original):\n",
    "            r\"\"\"\n",
    "            Given a Python string ``original``, remove unsupported characters, map characters\n",
    "            to integers and return a numpy array representing the processed string.\n",
    "            \"\"\"\n",
    "            # Create list of sentence's words w/spaces replaced by ''\n",
    "            result = ' '.join(original.translate(None, string.punctuation).lower().split())\n",
    "            result = result.replace(\" '\", \"\") # TODO: Deal with this properly\n",
    "            result = result.replace(\"'\", \"\")    # TODO: Deal with this properly\n",
    "            result = result.replace(' ', '  ')\n",
    "            result = result.split(' ')\n",
    "\n",
    "            # Tokenize words into letters adding in SPACE_TOKEN where required\n",
    "            result = np.hstack([SPACE_TOKEN if xt == '' else list(xt) for xt in result])\n",
    "\n",
    "            # Map characters into indicies\n",
    "            result = np.asarray([SPACE_INDEX if xt == SPACE_TOKEN else ord(xt) - FIRST_INDEX for xt in result])\n",
    "\n",
    "            # Add result to results\n",
    "            return result\n",
    "        \n",
    "        \n",
    "        #INTERVIEW:: This function explanation will be provided by Interviewer\n",
    "        def _get_log_power_spectrum(wav_file):\n",
    "            #All wav files are with 8k sampling rate : Taking Fourier representation: 20 ms speech to 81 feature\n",
    "            sample_rate = 8000\n",
    "            # load wave file with sampling rate 8000 which is already known. sr value is important\n",
    "            data, sr = librosa.load(wav_file, mono=True, sr=sample_rate)\n",
    "\n",
    "            #Short First Fourier transform - for every 20 second for 8k sampling rate= 160\n",
    "            stft = librosa.stft(data, n_fft=160, hop_length=160)\n",
    "\n",
    "            #np.abs(D[f, t]) is the magnitude of frequency bin f at frame t. Not taking the phase portion of it\n",
    "            amplitude = np.abs(stft)\n",
    "\n",
    "            #Compute dB relative to median power\n",
    "            log_power_spectrogram = librosa.power_to_db(amplitude**2, ref=np.median)\n",
    "            \n",
    "            return log_power_spectrogram\n",
    "\n",
    "        \n",
    "        def _load_power_spectrum(src_list):\n",
    "            txt_file, wav_file = src_list  # label, wave_file\n",
    "\n",
    "            #decode string to integer ------ This could be done without dictionary also --- Need to test the difference\n",
    "            # remove punctuation, to lower, clean white space \n",
    "            #sentence = ' '.join(open(txt_file).read().translate(None, string.punctuation).lower().split())\n",
    "            #print 'Sentence:', sentence\n",
    "            #lab = np.asarray([self.byte2index[ord(ch)] for ch in sentence])\n",
    "            label = ''\n",
    "\n",
    "            with codecs.open(txt_file, encoding=\"utf-8\") as open_txt_file:\n",
    "                label = unicodedata.normalize(\"NFKD\", open_txt_file.read()).encode(\"ascii\", \"ignore\")\n",
    "                label = text_to_char_array(label)\n",
    "            label_len = len(label)\n",
    "\n",
    "            feature = _get_log_power_spectrum(wav_file)\n",
    "            feature_len = np.size(feature, 1)\n",
    "\n",
    "            # return result\n",
    "            return label, label_len, feature, feature_len\n",
    "\n",
    "        \n",
    "        # enqueue function\n",
    "        def enqueue_func(sess, enqueue_op, data_producer):\n",
    "            # read data from source queue\n",
    "            \n",
    "            label_data_file_pair = sess.run(data_producer)\n",
    "          \n",
    "            train_label, train_label_length, train_wave_file, train_wave_file_len = _load_power_spectrum(label_data_file_pair)\n",
    "\n",
    "            sess.run(enqueue_op, feed_dict={label_input:train_label, label_input_length:train_label_length, feature_input:train_wave_file, feature_input_length:train_wave_file_len})\n",
    "                    \n",
    "                            \n",
    "\n",
    "        # load corpus\n",
    "        data_reader = DataReadAndValidate(data_path)       \n",
    "        labels, wave_files = data_reader.label, data_reader.wav\n",
    "        \n",
    "        # calc total batch count\n",
    "        self.num_batch = len(labels) // batch_size\n",
    "\n",
    "\n",
    "        \n",
    "        # to constant tensor\n",
    "        train_labels = tf.convert_to_tensor(labels, dtype=tf.string)\n",
    "        train_wave_files = tf.convert_to_tensor(wave_files, dtype=tf.string)\n",
    "\n",
    "        data_producer = tf.train.slice_input_producer([train_labels, train_wave_files], shuffle=True)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        number_of_threads = 3        \n",
    "        # 81 == Number of rows cols are variable length for speech\n",
    "        # Features are [81,None] vectors of floats\n",
    "        feature_input = tf.placeholder(tf.float32, shape=[81,None])\n",
    "        feature_input_length = tf.placeholder(tf.int32, shape=[])\n",
    "        \n",
    "        # Labels are integers of variable length.\n",
    "        label_input = tf.placeholder(tf.int32, shape=[None])\n",
    "        label_input_length = tf.placeholder(tf.int32, shape=[])        \n",
    "        \n",
    "        \n",
    "        padding_q = tf.PaddingFIFOQueue(16, [tf.int32, tf.int32, tf.float32, tf.int32], shapes=[[None],[],[81,None],[]])\n",
    "        enqueue_op = padding_q.enqueue([label_input, label_input_length, feature_input, feature_input_length])\n",
    "\n",
    "        runner = _FuncQueueRunner(enqueue_func, data_producer, padding_q, [enqueue_op] * number_of_threads)\n",
    "        \n",
    "        # register to global collection\n",
    "        tf.train.add_queue_runner(runner)\n",
    "      \n",
    "    \n",
    "        self.labels, self.labels_length, self.features, self.features_length = padding_q.dequeue_many(batch_size)\n",
    "    \n",
    "    \n",
    "        # print info\n",
    "        logging.vlog(0, \"SppechData corpus loaded.(total data=%d, total batch=%d)\", len(labels), self.num_batch)\n",
    "\n",
    "\n",
    "\n",
    "    def print_index(self, indices):\n",
    "        # transform label index to character\n",
    "        for i, index in enumerate(indices):\n",
    "            str_ = ''\n",
    "            for ch in index:\n",
    "                if ch > 0:\n",
    "                    str_ += unichr(self.index2byte[ch])\n",
    "                elif ch == 0:  # <EOS>\n",
    "                    break\n",
    "            print str_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test logic below->>>\n",
      "Base path from where data will be read: SpeechData/\n",
      "Loading the speech metadata from path: SpeechData/\n",
      "Load data from enabled directory with training flag on : ['SpeechData/train/1/', 'SpeechData/train/2/']\n",
      "3195\n",
      "Wrong media files:\n",
      "[]\n",
      "wav file length after validation\n",
      "3195\n",
      "Length CHeck Failed: \n",
      "[]\n",
      "Wrong transcription. contains numeric or special character\n",
      "[]\n",
      "set([])\n",
      "set([])\n",
      "3195\n",
      "3195\n",
      "Run Logic below->>>\n",
      "Closing\n"
     ]
    }
   ],
   "source": [
    "#Sample UNIT CODE TO TEST THE LOGIC\n",
    "\n",
    "#### Test Logic#####\n",
    "print 'Test logic below->>>'\n",
    "\n",
    "data = SpeechData()\n",
    "\n",
    "# Starting Graph Session------------>>>\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # initialize the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # initialize the queue threads to start to shovel data\n",
    "    coord = tf.train.Coordinator()\n",
    "\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "    try:\n",
    "\n",
    "        print ('Run Logic below->>>')\n",
    "\n",
    "        print ('Closing')\n",
    "\n",
    "    except Exception, e:\n",
    "        # Report exceptions to the coordinator.\n",
    "        coord.request_stop(e)\n",
    "    finally:\n",
    "        # stop our queue threads and properly close the session\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
